{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import os\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "#from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import glob\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "#Used to remove html markups, etc\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "#Functions used for preprocessing textual data\n",
    "\n",
    "def replace_contractions(text):\n",
    "    \"\"\"Replaces contractions (it's -> it is)\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    words = list(map(lambda x: unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore'), words))\n",
    "    return words\n",
    "\n",
    "\n",
    "\n",
    "def to_lowercase(words):\n",
    "    words = list(map(lambda x: x.lower(), words))\n",
    "    return words\n",
    "\n",
    "\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    words = list(map(lambda x: re.sub(r'[^\\w\\s]', '', x), words))\n",
    "    words = list(filter(None, words))#removing empty strings after punctuation removal\n",
    "    return words\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    words = [word for word in words if not word in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = list(map(lambda x: lemmatizer.lemmatize(x,pos='v'), words))\n",
    "    return words\n",
    "\n",
    "\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = lemmatize_verbs(words)\n",
    "    return words\n",
    "\n",
    "\n",
    "\n",
    "#Function that tracks occurrences of multi-worded terms\n",
    "\n",
    "\n",
    "def mult_replace(text):\n",
    "    dict = {\n",
    "        \"pro-rights\": \"prorights\",\n",
    "        \"heart attack\": \"heartattack\",\n",
    "        \"yellow-skinned\": \"yellowskinned\",\n",
    "        \"multi-racial\": \"multiracial\",\n",
    "        \"african-american\": \"africanamerican\",\n",
    "        \"afro-american\": \"afroamerican\",\n",
    "        \"non-white\": \"nonwhite\",\n",
    "        \"middle-east\": \"middleeast\",\n",
    "        \"middle-eastern\": \"middleeastern\",\n",
    "        \"cancer patient\": \"cancerpatient\",\n",
    "        \"drug addict\": \"drugaddict\",\n",
    "        \"split-personality\": \"splitpersonality\",\n",
    "        \"Pro-rights\": \"prorights\",\n",
    "        \"Heart attack\": \"heartattack\",\n",
    "        \"Yellow-skinned\": \"yellowskinned\",\n",
    "        \"Multi-racial\": \"multiracial\",\n",
    "        \"African American\": \"africanamerican\",\n",
    "        \"Afro-American\": \"afroamerican\",\n",
    "        \"Non-white\": \"nonwhite\",\n",
    "        \"Middle-east\": \"middleeast\",\n",
    "        \"Middle-eastern\": \"middleeastern\",\n",
    "        \"Cancer patient\": \"cancerpatient\",\n",
    "        \"Drug addict\": \"drugaddict\",\n",
    "        \"Split-personality\": \"splitpersonality\",\n",
    "        \"ex-criminal\": \"excriminal\",\n",
    "        \"Ex-criminal\": \"excriminal\",\n",
    "        \"blue-collar\": \"bluecollar\",\n",
    "        \"Blue-collar\": \"bluecollar\",\n",
    "        \"working-class\": \"workingclass\",\n",
    "        \"Working-class\": \"workingclass\",\n",
    "        \"Native American\": \"nativeamerican\",\n",
    "        \"Indo-Aryan\": \"indoaryan\",\n",
    "        \"Cross-dressing\": \"crossdressing\",\n",
    "        \"cross-dressing\": \"crossdressing\"\n",
    "    }\n",
    "    \n",
    "    regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "    \n",
    "    return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = []\n",
    "\n",
    "file_list = glob.glob(\"/media/nishanthsanjeev/2CC66AE5C66AAF30/Embeddings/COCA/Text Files/*/*1997.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/media/nishanthsanjeev/2CC66AE5C66AAF30/Embeddings/COCA/Text Files/text_acad_isi/text_acad_1997.txt',\n",
       " '/media/nishanthsanjeev/2CC66AE5C66AAF30/Embeddings/COCA/Text Files/text_fic_jjw/text_fic_1997.txt',\n",
       " '/media/nishanthsanjeev/2CC66AE5C66AAF30/Embeddings/COCA/Text Files/text_mag_jkf/text_mag_1997.txt',\n",
       " '/media/nishanthsanjeev/2CC66AE5C66AAF30/Embeddings/COCA/Text Files/text_news_nne/text_news_1997.txt',\n",
       " '/media/nishanthsanjeev/2CC66AE5C66AAF30/Embeddings/COCA/Text Files/text_spok_yuv/text_spok_1997.txt',\n",
       " '/media/nishanthsanjeev/2CC66AE5C66AAF30/Embeddings/COCA/Text Files/text_tvm_nwh/text_tvm_1997.txt']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/nishanthsanjeev/2CC66AE5C66AAF30/Embeddings/COCA/Text Files/text_acad_isi/text_acad_1997.txt\n",
      "/media/nishanthsanjeev/2CC66AE5C66AAF30/Embeddings/COCA/Text Files/text_fic_jjw/text_fic_1997.txt\n",
      "/media/nishanthsanjeev/2CC66AE5C66AAF30/Embeddings/COCA/Text Files/text_mag_jkf/text_mag_1997.txt\n",
      "/media/nishanthsanjeev/2CC66AE5C66AAF30/Embeddings/COCA/Text Files/text_news_nne/text_news_1997.txt\n",
      "/media/nishanthsanjeev/2CC66AE5C66AAF30/Embeddings/COCA/Text Files/text_spok_yuv/text_spok_1997.txt\n",
      "/media/nishanthsanjeev/2CC66AE5C66AAF30/Embeddings/COCA/Text Files/text_tvm_nwh/text_tvm_1997.txt\n"
     ]
    }
   ],
   "source": [
    "#Generating the new, combined text file.\n",
    "\n",
    "\n",
    "with open('coca_1997.txt', 'w') as outfile:\n",
    "    for names in file_list:\n",
    "        print(names)\n",
    "        with open(names,'r') as infile:\n",
    "            outfile.write(infile.read())\n",
    "            \n",
    "        outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\n4000054 Section : Features Research sheds new light on the origin of humanity's most intimate quadruped ally  The poor dog ,  wrote poet Lord Byron in a flight of emotion ,  in life the firmest friend , The first to welcome , foremost to defend .\",\n",
       " 'And certainly , few animal lovers would care to differ .',\n",
       " \"The dog , after all , is commonly referred to as man's best friend , and unquestionably serves a wide range of human purposes .\",\n",
       " 'Thanks to artificial selection , there are dogs that guard houses and dogs that herd livestock , dogs that locate game birds for shooting and dogs that retrieve game birds that have been shot , dogs that pull sleds and dogs that sit languidly in human laps .',\n",
       " 'Clearly , the relationship between dog and human runs deep in our culture and our psyches .']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#begin_time = datetime.datetime.now()\n",
    "\n",
    "with open('coca_1997.txt','r') as f:\n",
    "    strt = f.read()\n",
    "    \n",
    "str1 = mult_replace(strt)\n",
    "del strt\n",
    "\n",
    "comb_cl = re.sub(\"\\\\[']\\d*\",\"'\",str1)\n",
    "del str1\n",
    "\n",
    "comb_cl_denoised = denoise_text(comb_cl)\n",
    "del comb_cl\n",
    "print(datetime.datetime.now() - begin_time)\n",
    "comb_cl_denoised = comb_cl_denoised.replace('@','')\n",
    "\n",
    "comb_cl_denoised_new = comb_cl_denoised.replace(\" '\",\"'\")\n",
    "del comb_cl_denoised\n",
    "\n",
    "comb_cl_den_new2 = comb_cl_denoised_new.replace(\" n't\",\"n't\")\n",
    "del comb_cl_denoised_new\n",
    "\n",
    "\n",
    "comb_cl_new3 = comb_cl_den_new2.replace(\"-\",\" \")\n",
    "del comb_cl_den_new2\n",
    "\n",
    "\n",
    "comb_cl_d = comb_cl_new3.replace(\"\\\"\",\"\")\n",
    "\n",
    "sentences = nltk.sent_tokenize(comb_cl_d)\n",
    "del comb_cl_d\n",
    "\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing contractions, i.e. it's -> it is, et cetera\n",
    "\n",
    "\n",
    "sentences_mod = list(map(lambda x: replace_contractions(x), sentences))    \n",
    "del sentences\n",
    "\n",
    "\n",
    "words = list(map(lambda x: nltk.word_tokenize(x), sentences_mod))    \n",
    "    \n",
    "del sentences_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['4000054',\n",
       "  'section',\n",
       "  'feature',\n",
       "  'research',\n",
       "  'shed',\n",
       "  'new',\n",
       "  'light',\n",
       "  'on',\n",
       "  'the',\n",
       "  'origin',\n",
       "  'of',\n",
       "  'humanity',\n",
       "  's',\n",
       "  'most',\n",
       "  'intimate',\n",
       "  'quadruped',\n",
       "  'ally',\n",
       "  'the',\n",
       "  'poor',\n",
       "  'dog',\n",
       "  'write',\n",
       "  'poet',\n",
       "  'lord',\n",
       "  'byron',\n",
       "  'in',\n",
       "  'a',\n",
       "  'flight',\n",
       "  'of',\n",
       "  'emotion',\n",
       "  'in',\n",
       "  'life',\n",
       "  'the',\n",
       "  'firmest',\n",
       "  'friend',\n",
       "  'the',\n",
       "  'first',\n",
       "  'to',\n",
       "  'welcome',\n",
       "  'foremost',\n",
       "  'to',\n",
       "  'defend'],\n",
       " ['and',\n",
       "  'certainly',\n",
       "  'few',\n",
       "  'animal',\n",
       "  'lovers',\n",
       "  'would',\n",
       "  'care',\n",
       "  'to',\n",
       "  'differ'],\n",
       " ['the',\n",
       "  'dog',\n",
       "  'after',\n",
       "  'all',\n",
       "  'be',\n",
       "  'commonly',\n",
       "  'refer',\n",
       "  'to',\n",
       "  'as',\n",
       "  'man',\n",
       "  's',\n",
       "  'best',\n",
       "  'friend',\n",
       "  'and',\n",
       "  'unquestionably',\n",
       "  'serve',\n",
       "  'a',\n",
       "  'wide',\n",
       "  'range',\n",
       "  'of',\n",
       "  'human',\n",
       "  'purpose'],\n",
       " ['thank',\n",
       "  'to',\n",
       "  'artificial',\n",
       "  'selection',\n",
       "  'there',\n",
       "  'be',\n",
       "  'dog',\n",
       "  'that',\n",
       "  'guard',\n",
       "  'house',\n",
       "  'and',\n",
       "  'dog',\n",
       "  'that',\n",
       "  'herd',\n",
       "  'livestock',\n",
       "  'dog',\n",
       "  'that',\n",
       "  'locate',\n",
       "  'game',\n",
       "  'bird',\n",
       "  'for',\n",
       "  'shoot',\n",
       "  'and',\n",
       "  'dog',\n",
       "  'that',\n",
       "  'retrieve',\n",
       "  'game',\n",
       "  'bird',\n",
       "  'that',\n",
       "  'have',\n",
       "  'be',\n",
       "  'shoot',\n",
       "  'dog',\n",
       "  'that',\n",
       "  'pull',\n",
       "  'sled',\n",
       "  'and',\n",
       "  'dog',\n",
       "  'that',\n",
       "  'sit',\n",
       "  'languidly',\n",
       "  'in',\n",
       "  'human',\n",
       "  'lap'],\n",
       " ['clearly',\n",
       "  'the',\n",
       "  'relationship',\n",
       "  'between',\n",
       "  'dog',\n",
       "  'and',\n",
       "  'human',\n",
       "  'run',\n",
       "  'deep',\n",
       "  'in',\n",
       "  'our',\n",
       "  'culture',\n",
       "  'and',\n",
       "  'our',\n",
       "  'psyches']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_data = normalize(words)\n",
    "train_data = list(map(lambda x: normalize(x), words))    \n",
    "del words\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/nishanthsanjeev/Harvard/PickledCOCA/train1997','wb') as f:\n",
    "    pickle.dump(train_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('/home/nishanthsanjeev/Harvard/PickledCOCA/train1993','rb') as fr:\n",
    "#     train_data = pickle.load(fr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100\n",
    "window_size = 40\n",
    "min_word = 5\n",
    "down_sampling = 1e-2\n",
    "\n",
    "begin_time = datetime.datetime.now()\n",
    "\n",
    "ft_model = FastText(train_data,\n",
    "                      size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                    sg=1,\n",
    "                      iter=10,\n",
    "                   workers = 6)\n",
    "\n",
    "\n",
    "print(datetime.datetime.now() - begin_time)#time taken to train this set of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchen:['kitchenette', 'bathroom', 'upstairs', 'downstairs', 'breakfast']\n",
      "death:['torture', 'dead', 'murder', 'innocent', 'mortally']\n",
      "king:['larry', 'peter', 'jester', 'sain', 'haing']\n",
      "queen:['queenie', 'princesse', 'princess', 'royal', 'bessette']\n",
      "strong:['clearly', 'tough', 'hard', 'trouble', 'surely']\n",
      "weak:['negatively', 'weakness', 'weaker', 'weakling', 'negative']\n",
      "woman:['girl', 'daughter', 'child', 'she', 'matronly']\n",
      "man:['one', 'ever', 'way', 'who', 'thing']\n"
     ]
    }
   ],
   "source": [
    "semantically_similar_words = {words: [item[0] for item in ft_model.wv.most_similar([words], topn=5)]\n",
    "                  for words in ['kitchen', 'death', 'king', 'queen', 'strong', 'weak','woman','man']}\n",
    "\n",
    "for k,v in semantically_similar_words.items():\n",
    "    print(k+\":\"+str(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.wv.save_word2vec_format(\"/home/nishanthsanjeev/Harvard/COCA_Vectors_Year/coca_vector1997.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ft_model = KeyedVectors.load_word2vec_format(\"coca_vector1991.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
